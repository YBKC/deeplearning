3.신경망


3.1.1 신경망의 예

입력층 -> 은닉층 -> 출력층 


3.1.2 퍼셉트론 복습

b는 편향, 	w1, w2는 가중치를 나타내는 매개변수로 각 신호의 영향력을 제어
	
	0 (b + w1x1 + w2x2 < 0)
y = {
	1 (b + w1x1 + w2x2 > 0)


3.1.3 활성화 함수의 등장

a = b + w1x1 + w2x2
y = h(a) 

그림 3.4를 보는게 이해해 좋다 ( 67page) 

이 활성화 함수 h(a)가 퍼셉트론에서 신경망으로 가기 위한 길잡이가 된다.


3.2 활성화 함수

식 3.3 
		0
h(x) = {              임계값을 기준으로 출력이 바뀐다. 이를 계단 함수라고 표현하기도 한다.
		1

그렇다면 h(x) 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있다.
그렇다면 계단 함수 이외의 함수를 사용하면 어떻게 될까? 
활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠


3.2.1 시그모이드 함수

h(x) = 1
	——
	 1 + exp(-x)


3.2.2 계단 함수 구현하기

>>> def step_function(x):
...     y = x > 0
...     return y.astype(np.int)
... 
>>> import numpy as np
>>> x = np.array([-1.0, 1.0, 2.0])
>>> x
array([-1.,  1.,  2.])
>>> y = x > 0
>>> y
array([False,  True,  True])
>>> y = y.astype(np.int)
>>> y
array([0, 1, 1])


3.2.5 시그모이드 함수와 계단 함수 비교

* 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력
* 입력이 작거나 커도 출력은 0에서 1 사이라는 것이 둘의 공통점

3.2.6 비선형 함수 

선형 함수는 f(x) = ax + b이고 이떄, a와 b는 상수입니다. 그래서 선형 함수는 곧은 1개의 직선이 됩니다.
한편, 비선형 함수는 문자 그대로 ‘선형이 아닌’ 함수 입니다. 직선 1개로 그릴 수 없는 함수를 말합니다.

왜 선형함수는 안될까? 신경망 층을 깊게 하는 의미가 없다.
h(x) = cx라고 가정,   y = c * c * c * x 곱셈을. 세번 실행하지만, 실은 y(x) = ax와 똑같은 식입니다. a = c의 3승과 같은 의미

그래서 값이 변동되는, 여러 층으로 구성되는 이점을 살리고 싶다면, 활성화 함수로는 반드시 비선형 함수가 필요합니다.


3.3 다차원 배열의 계산


3.3.2 행렬의 내적(행렬 곱)

(1 2)   (5 6)   (19 22)
(3 4)   (7 8)   (43 50)

행렬 특성상 

(2,2) * (2,2) = (2,2)
(3행 2열) * (2행 1열) = (3행 1열)로 값이 떨어진다.
계산 가능 여부는 앞의 행렬의 열과 뒤에 행렬의 행의 갯수가 같아야함

3.3.3 신경망의 내적

행렬의 곱으로 신경망의 계산을 수행한다
82page 그림 참조

numpy.dot(X,W) //는 Y
함수가 신경망 구현에 행렬 곱셈에 큰 역할을 한다.


3.4 3층 신경망 구현하기

3.4.3 구현정리 

행렬 계산, sigmoid 함수등을 이용하면 신경망을 구현할 수 있다.
신경망 구현의 관례에 따라 가중치 W1과 같이 대문자로 쓰고 편향과 중간 결과등은 모두 소문자
자세한 건 코드를 봐라 (89 page)


3.5 출력층 설계하기

신경망은 분류외 회귀 모두에 이용할 수 있다.
회귀에는 항등 함수
분류에는 소프트맥스 함수

* 분류(classfication
    * 분류는 데이터가 어느 class에 속하느냐는 문제이다. 
* 회귀(regression)
    * 회귀는 입력 데이터에서 (연속적인) 수치를 예측하는 문제이다.
    * ex 사진 속 인물의 몸무게(57.4kg?)을 예측하는 문제가 회귀이다.

3.5.1 항등 함수와 소프트맥스 함수 구현하기
* 항등함수는 입력을 그대로 출력한다.
* 소프트 맥스 함수는
    * yk = exp(a)/ 시그마(i =1부터 n까지) exp(ai)

3.5.3 소프트맥스 함수의 특징
* 소프트 맥스 함수의 출력은 0에서 1.0사이의 실수 입니다.
* 함수의 출력의 총합은 1이 된다. 출력 총합이 1이 된다는 점이 소프트 맥스 함수의 중요한 성질
* 이 성질 덕분에 소프트 맥스 함수를 이용하면 확률을 나타낼 수 있다.
    * 가령 0.018, 0.245, 0.737 로 값을 내었다면, 1%. 확률로 0번째 class, 24% 확률로 1번째 class, 75%확률로 2번쨰 class라고 확률적인 결론도 낼 수 있다.
    * 대소 관계는 변하지 않다. 
    * 신경망에서는 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식해, 소프트 맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않으므로, 생략해도 된다. 
    * 학습과 추론에서 추론은 소프트맥스를 생략하지만, 학습과정에서는 소프트맥스 함수를 사용한다. (4장 참조)

3.5.4 출력층의 뉴런 수 정하기

* 출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야 합니다. 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적입니다.
* 예를 들어 0부터 9로 분류하고 싶다면 출력층의 뉴런 갯수는 10개로 설정하는게 적절하다.

3.6 손글씨 숫자 인식하기

* 이미 학습된 매개변수를 사용해 학습과정은 생략하고, 추론 과정만 구현, 이 추론 과정을 신경망의 순전파 라고 한다.

3.6.2 신경망의 추론 처리
* 현업에서도 신경망에 전처리를 활발히 사용합니다. 전처리를 통해 식별 능력을 개선, 학습 속도를 높인다. 예를 들어 전체 평균, 표준편차를 이용하는 데이터들이 0을 중심으로 분포하도록 이동하거나, 데이터 확산 범위를 제한하는 정규화를 수행, 데이터를 균일하게 분포시키는 데이터 백색화 등도 있습니다.  

정리.

신경망은 각 층의 뉴런들이 다음 층의 뉴런으로 신호를 전달한다는 점에서 앞 장의 퍼셉트론과 같다.
다음 뉴런으로 갈 때 신호를 변화시키는 활성화 함수에 큰 차이가 있다. 신경망에서는 매끄럽게 변화하는 시모이드 함수를, 퍼셉트론에서는 갑자기 변화하는 계단 함수를 활성화 함수로 사용, 이 차이가 신경망 학습에 중요합니다. 

